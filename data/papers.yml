- title: "On the Language and Gender Biases in PSTN, VoIP and Neural Audio Codecs"
  category: "Speech"
  date: "2025-10-21"
  link: "https://www.isca-archive.org/interspeech_2025/altwlkany25_interspeech.pdf"
  note: >
    電話用の音声コーデック（PSTN, A-lawやμ-law）、VoIP用の音声コーデック（Opus）、ニューラル音声コーデック（Encodec, DAC）における言語・性別バイアスを評価
    VoxForgeの17言語、約17万発話を各コーデックで圧縮・復元し、音質をViSQOLで評価
    実験では、PSTNは性別間、NACは言語間のバイアスが大きく、またどのコーデックでも一貫して男性の方が高音質に復元される傾向があることを確認
    ViSQOLのスコアが言語や性別等に依存する可能性については言及されておらず、圧縮・復元を介さない原音声に対する評価結果も見てみたい

- title: "Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models"
  category: "Multimodal (T/S)"
  date: "2025-10-20"
  link: "https://arxiv.org/abs/2510.12116"
  note: >
    Encoder-Adapter-LLM型の音声言語モデルにおいて、音声入力の場合にはテキスト入力の場合よりも応答品質が低下する「モダリティギャップ」が存在
    このギャップはテキスト入力時/音声入力時の平均特徴量のコサイン類似度やユークリッド距離と相関があることを発見し、実際にテキスト入力時の特徴量に近づけるような介入を行うことで応答精度を改善できることを確認

- title: "LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution Designed for Speech Large Language Models"
  category: "NAC"
  date: "2025-10-19"
  link: "https://github.com/meituan-longcat/LongCat-Audio-Codec/blob/main/tech_report.pdf"
  note: >
    低ビットレートで高い音質を実現する音声トークナイザーLongCat-Audio-Codecを提案
    EncoderはTransformerベースのsemantic encoderの特徴量をk-meansクラスタリングして得るsemantic tokenと、畳み込みベースのacoustic encoderの特徴量にAdaptive Grouped RVQを適用して得るacoutic tokensを生成
    Decoderはストリーミング可能なようLSTMとCausal Convで構成される
    実験では同ビットレート帯の既存手法を上回る品質（特にWER）を達成し、Decoderのみを微調整することで特定話者に対する話者類似性をさらに改善できることも示した

- title: "SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality Evaluation"
  category: "Multimodal (T/S)"
  date: "2025-10-18"
  link: "https://arxiv.org/abs/2510.14664"
  note: >
    Qwen2.5-Omniベースの音声品質評価に特化したLLM、SQ-LLMと、そのための約3万件の音声と12万件のアノテーションを含むデータセットSpeechEvalを提案
    SpeechEvalデータセットは英中日仏の4言語、実音声・合成音声の両方から構成され、人手でのアノテーション（選択式）をもとにLLMで生成された評価文章を人手で再度チェックする複数段階のプロセスで作成
    SQ-LLMは (1) 音声品質評価 (2) 2つの音声の品質比較 (3) 音声品質改善案の生成 (4) Deepfake検出 の4つのタスクで学習
    明瞭性や話速、主観的体験など8項目のスコアを<think>タグ内で予測するCoTと、Qwen3を用いて評価結果の文章を評価するGRPOを組み合わせることで、高い精度での音声品質評価を実現

- title: 'Do LLMs "Feel"? Emotion Circuits Discovery and Control'
  category: "LLM"
  date: "2025-10-17"
  link: "https://arxiv.org/abs/2510.11328"
  note: >
    LLMの内部には感情を表現する特定のニューロンやアテンションヘッド（感情回路）が存在することを発見し、それを用いた高精度な感情制御手法を提案
    従来のステアリングベクトルに基づく感情操作（中間層に感情ベクトルを加える）と比較して、回路ベースの手法は特に「驚き」の感情を中心により正確な制御を実現
    実験では、回路操作によって意図した感情を約99.6％の確率で正確に表現でき、感情のニュアンスや表現力も向上することを確認

- title: "ZIPA: A family of efficient models for multilingual phone recognition"
  category: "Speech"
  date: "2025-10-16"
  link: "https://aclanthology.org/2025.acl-long.961.pdf"
  note: >
    ZipformerとCTCまたはRNN-Tを組み合わせたIPA認識モデルZIPAと、そのためのデータセットIPAPack++を提案
    IPAPack++はCommon Voice, LibriSpeech, MLS, AIShell-1などを含む1万7千時間88言語の音声と、CharsiuG2PおよびEpitranによる自動IPAアノテーションからなるコーパスであり、IPAPackよりも統一された表現を採用
    ZIPAはWhisper Encoderのような事前学習モジュールを用いる代わりにZipformerをスクラッチから学習することで軽量・高速化しつつ、CTC版はConsistency-Regularized CTCを用いることで精度を改善
    実験では既存の音素認識モデルよりも高い精度を達成し、既知言語ではRNN-T版、未知言語ではCTC版が特に有効であることを確認
    一方で学習データのIPAがG2P依存であることに起因し、実際に発話されたIPAよりもテキストに対応するIPAが優先される傾向にある (L2等で顕著) など、課題も指摘されている

- title: "The taste of IPA: Towards open-vocabulary keyword spotting and forced alignment in any language"
  category: "Speech"
  date: "2025-10-16"
  link: "https://aclanthology.org/2024.naacl-long.43.pdf"
  note: >
    IPAを用いた多言語キーワードスポッティングCLAP-IPAおよび強制アライメント手法IPA-Alignerと、そのためのデータセットIPAPackを提案
    IPAPackはFLEURS, Multilingual Spoken Words Corpus, DoReCoの3つのデータセットにIPAアノテーションを付与した115言語1000時間のコーパス
    実験では特定言語特化のモデルには一部で劣るものの高い精度を達成し、特にテキストベースのモデルが苦手とする未知言語に対しても対応できることを確認

- title: "IndicSynth: A Large-Scale Multilingual Synthetic Speech Dataset for Low-Resource Indian Languages"
  category: "Dataset (Speech)"
  date: "2025-10-15"
  link: "https://aclanthology.org/2025.acl-long.1070.pdf"
  note: >
    声のなりすましを防ぐAudio Deepfake Detection (ADD) の研究は英語や中国語に偏っており、インドのような多言語・低リソースな環境では十分な精度でない
    この課題に対し、TTS (XTTS-v2) やVC (FreeVC24) を用いて約4000時間、12言語からなる合成音声データセットIndicSynthを作成
    既存の言語識別モデルや話者認証モデルを用いた実験では、IndicSynthに含まれる合成音声が言語、話者ともに高い精度で模倣できていることを確認

- title: "Full-Duplex-Bench-v2: A Multi-Turn Evaluation Framework for Duplex Dialogue Systems with an Automated Examiner"
  category: "Multimodal (T/S)"
  date: "2025-10-14"
  link: "https://arxiv.org/abs/2510.07838"
  note: >
    gpt-realtimeを用いたfull-duplex音声対話システムの評価用ベンチマークFull-Duplex-Bench-v2を提案
    本ベンチマークはマルチターン対話に焦点を当て、gpt-realtime側 (Examiner) がシステム側 (Evaluatee) に話しかけながら一連のタスクを完遂する形式を採用
    タスクは日常会話、以前の情報の修正、指示語の追従、安全性の4つのカテゴリに分かれ、ターンテイキングの円滑さ、マルチターンでの指示追従、タスク固有の評価指標の3軸で評価
    gpt-realtime, Moshi, Freeze-Omniを比較した実験ではgpt-realtimeが一貫して最も高性能であり、またExaminer側からの遮りや相槌を行わないslowな設定の方が全体的な性能が向上することも確認
    タスクの多様化、音響情報の加味（感情や同調等）、多言語への対応などが今後の展望として挙げられている

- title: "Byte Latent Transformer: Patches Scale Better Than Tokens"
  category: "LLM"
  date: "2025-10-13"
  link: "https://aclanthology.org/2025.acl-long.453.pdf"
  note: >
    初めてトークナイザーベースのLLMと匹敵するバイトベースLLMとしてByte Latent Transformer (BLT)を提案
    入力バイト列は次バイトの予測の難しさ（エントロピー）を元に動的にパッチングされ、Latent Global Transformerがパッチ化された潜在表現に対して駆動
    BLTは高いスケーリング特性を持ち、さらにトークナイザーベースのLLMが苦手とするノイジーな入力・G2P・文字理解タスクにも頑健

- title: "Mamba-3: Improved Sequence Modeling using State Space Principles"
  category: "LLM"
  date: "2025-10-12"
  link: "https://openreview.net/pdf?id=HwCvaJOiCj"
  note: >
    Mamba-2を性能やハードウェア効率の面で改良したMamba-3を提案
    (1) オイラー法に基づく離散化を台形近似に変更することで誤差を抑制 (2) 状態を複素数で扱うSSMを導入しつつ、等価な実数値SSMに変換 (3) 入出力を多次元化することで演算強度を向上
    実験では一貫してMamba-2を上回る性能を達成、一般的なタスクではTransformerをも上回るが、retrievalではやや劣る結果

- title: "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models"
  category: "Multimodal (T/S)"
  date: "2025-10-11"
  link: "https://arxiv.org/abs/2510.06917"
  note: >
    ユーザーの発話を4秒程度のチャンクで受け取りながら、各チャンクに対してreasoningを行い、ユーザーの発話終了すぐに応答を生成し始める音声対話モデルSHANKSを提案
    これまであまり扱われてこなかった「ユーザーが長く話す」「ユーザーが誤った情報を発した場合、積極的に遮る」といった設計を採用し、数学の問題解決を援助するなどに応用
    さらに、reasoningの中ではツール呼び出しも実行でき、ユーザーの発話中にツール呼び出し + 成功しなかったもののみ発話終了後に再呼び出し することで高い精度を達成

- title: "Less is More: Recursive Reasoning with Tiny Networks"
  category: "ML"
  date: "2025-10-10"
  link: "https://arxiv.org/abs/2510.04871"
  note: >
    Hierarchical Reasoning Modelは、駆動頻度の異なる2つの小さなネットワークを用いて高い推論能力を実現したが、勾配計算における過度な近似、脳との類似性に基づく複雑なモデル設計等の課題があった
    本研究ではネットワークを1つに減らし、反復推論の流れも簡略化しつつ（高頻度でn回駆動→低頻度で1回駆動）、複雑な推論タスクにおける性能を改善
    数独、迷路、ARC-AGI、ARC-AGI2の4つのベンチマークでo3-mini-highやGemini 2.5 Proを上回る性能を達成

- title: "Latent Speech-Text Transformer"
  category: "Multimodal (T/S)"
  date: "2025-10-09"
  link: "https://arxiv.org/abs/2510.06195"
  note: >
    テキストとHuBERTトークンの両方を扱えるSpirit-LM型の音声言語モデルに対し、音声トークンを動的にパッチング/展開する機構を導入したLSTを提案
    (1) 固定長の静的パッチング (2) アラインメントベースの動的パッチング (3) (1)と(2)のランダム (4) (2)→(1)へのカリキュラム学習 の4通りを比較し、(4)が最も性能が良いことを確認
    また、音声系列のパッチングを行わないベースラインと比較しても安定して性能が高く、特に音声入出力QAで有望

- title: "KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI"
  category: "Multimodal (T/S)"
  date: "2025-10-08"
  link: "https://arxiv.org/abs/2510.02327"
  note: >
    Speech-to-speechの形で動作するフロントエンドLMにバックエンドLLMを統合したKnowledge-Access Model Extension: KAMEを提案
    バックエンドLLMは部分的な書き起こしを元に応答を生成し、Moshi-likeなフロントエンドLMはその暫定的な応答も用いて応答音声を生成
    フロントエンドLMの学習には、テキストQAデータセットにおける実際の応答をヒントとして合成された暫定的な応答を用いた
    実験ではMoshiと同等の遅延でMT-Benchにおけるスコアを大幅に改善し、バックエンドLLMの知識活用の有効性を確認
    一方で完全なカスケードシステムであるUnmuteと比較すると応答品質はやや劣っており、部分的な書き起こしに基づく応答生成の限界が示唆されている

- title: "Self-Improvement in Multimodal Large Language Models: A Survey"
  category: "Multimodal (T/I)"
  date: "2025-10-07"
  link: "https://arxiv.org/abs/2510.02665"
  note: >
    VLMにおける自己改善（自動化された手法でデータ収集・前処理を行いモデルを改善する手法）に関するサーベイ
    データ収集・データ前処理・モデル改善の3つの観点から既存手法を整理し、各段階での課題と今後の展望を議論
    また指示学習、嗜好学習、RLVRに利用可能な合成データセットについても言及

- title: "ExGRPO: Learning to Reason from Experience"
  category: "LLM"
  date: "2025-10-06"
  link: "https://arxiv.org/abs/2510.02245"
  note: >
    既存のon-policy型RL手法（GRPO等）は、生成したサンプルを一度更新に利用したら破棄するため、再利用性や訓練安定性に制約があった
    本研究ではexperience replayを導入したExGRPOを提案し、(1) 正解率 (中難易度を重視) (2) エントロピー (低いほど高品質な推論) をもとに経験を抽出することで効率的な学習を実現
    1.5Bから8Bの様々なモデルで有効性を確認し、特に数学系ベンチマークで大きな改善を達成

- title: "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation"
  category: "LLM"
  date: "2025-10-03"
  link: "https://arxiv.org/abs/2509.25849"
  note: >
    GRPOは簡単すぎる/難しすぎるタスク（あるプロンプトに対する全てのサンプルで正解/不正解が揃う場合）に対して学習信号が得られない課題がある
    本研究ではまず成功確率pを用いて有効な学習信号を得るために必要なサンプル数を定式化し、p<0.01のようなケースでは平均で100サンプル以上が必要であることを示した
    そこで、全計算資源をナップザックの容量、各プロンプトに対して配分する計算資源（=サンプル生成数）重さ、その問題が解けることによる利益を価値として、計算資源配分を動的に決定するKnapsack RLを提案
    通常のGRP0と比較して特に数学を中心としたSTEM系ベンチマークで一貫して性能を改善し、学習中の有効な学習信号の割合も高く維持できることを実証

- title: "Sora 2 System Card"
  category: "Multimodal Generation"
  date: "2025-10-01"
  link: "https://cdn.openai.com/pdf/50d5973c-c4ff-4c2d-986f-c72b5d0ff069/sora_2_system_card.pdf"
  note: >
    高品質な動画と音を生成するOpenAIのモデルSora 2のシステムカード
    物理法則やフォトリアル、音と映像の同期、テキスト追従、表現の幅を強化
    システムカードの方は安全性に関する記述が多く、データセットの厳格なフィルタリング、利用時の入出力チェック、年齢制限、ウォーターマーク付与、レッドチームの監視等が言及されている

- title: "SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS"
  category: "TTS"
  date: "2025-09-30"
  link: "https://arxiv.org/abs/2509.20802"
  note: >
    TransformerベースのTTSモデルに対して、重要度の低い層の削除と知識蒸留を組み合わせ、効率的なモデル圧縮を実現する手法SPADEを提案
    従来プルーニングにおいて各層の重要性は入出力のcosine類似度で測られていたが、SPADEでは特定の層を削除した際のWERの低下から直接的に測ることで性能劣化を抑制
    また、連続する複数層の削除にあたって、削除する最後の層の中間特徴量やアテンション重みとの誤差を最小化する知識蒸留を行うことで、後段への影響を抑えた
    CosyVoice 2に適用した実験ではわずかな性能劣化で40%のパラメータ削減・推論高速化を達成したが、LLaSAに適用した場合はやや性能劣化が大きく、全層の重要性が高い場合の適用に課題が残る
    事前学習時の5%程度のデータで蒸留できるのは魅力的だが、削減後の規模のモデルを事前学習した場合とどちらが性能が良いかは気になる

- title: "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution"
  category: "ML"
  date: "2025-09-29"
  link: "https://arxiv.org/abs/2509.19349"
  note: >
    LLMを進化的なプログラム探索に組み込む研究では、サンプル効率が悪く膨大な世代数が必要になる課題があった
    ShinkaEvolveは (1) 親サンプリングにおける探索と活用の制御 (2) LLMを用いたコードの新規性に基づく棄却サンプリング (3) バンディット方式によるLLM選択戦略 の3つを組み合わせてサンプル効率を改善
    実験では150サンプルで従来を上回るcircle packing解を発見するなど、サンプル効率と汎用性を両立する結果を確認

- title: "Building Tailored Speech Recognizers for Japanese Speaking Assessment"
  category: "ASR"
  date: "2025-09-28"
  link: "https://arxiv.org/abs/2509.20655"
  note: >
    音声からアクセント付き音素列を予測する日本語特化の音声認識システムを提案
    Mimi Encoderを用いてストリーミングで抽出された音声特徴量を元にTransformerでテキスト、fo、phonetic alphabet (PA) の3つを同時に予測（マルチタスク学習）
    音素情報のついた学習データが少なく（CSJ Core）、直接予測されたPAは精度が不十分なため、テキストと語彙群から推定されたPAとの間でlattice fusionを行い精度を改善
    CSJ, JSUTでの評価でWhisperやMultiPAを大きく上回る性能を達成

- title: "Qwen3-Omni Technical Report"
  category: "Multimodal (T/S/I/V)"
  date: "2025-09-27"
  link: "https://arxiv.org/abs/2509.17765"
  note: >
    Qwen2.5-Omniを改良し、テキスト・音声・画像・動画の全モダリティで同規模のモダリティ特化モデルと同等の性能を実現
    大小2つのLLMを用いるThinker-Talker構造にMoEを導入し、音声生成は12.5Hz x 16コードブックを用いたストリーミング生成で理論上の遅延234msを達成
    また音声エンコーダはWhisper large-v3をベースにしていたQwen2.5-Omniと異なり、650Mパラメータのエンコーダを2千万時間の音声データで一から教師あり学習

- title: "VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency"
  category: "TTS"
  date: "2025-09-26"
  link: "https://www.arxiv.org/abs/2509.15969"
  note: >
    ストリーミング可能な音素入力を基にMimiの音声トークンを逐次生成する入出力ストリーミングTTS、VoXtreamを提案
    モデルは(1) 音素を埋め込むPhoneme Transformer、(2) (1)の出力からMimiの1コードブック目と音素継続長を予測するTemporal Transformer、(3) (2)の出力からMimiの2~12コードブック目を予測するDepth Transformer の3つからなる
    入出力ストリーミング条件ではCosyVoice 2を超える品質を達成し、first-packet latencyは102ms, RTFはtorch.compile下で0.17を達成
    話者類似性がCosyVoice 2に及ばない点、G2PやMFAなどの外部依存、G2Pが将来のコンテキストに依存する点が課題か

- title: "Language Models Resist Alignment: Evidence From Data Compression"
  category: "LLM"
  date: "2025-09-25"
  link: "https://aclanthology.org/2025.acl-long.1141.pdf"
  note: >
    アラインメントされたLLMであっても少量の追加学習によって元の非アラインメント状態に戻せるという「LLMのアラインメントへの抵抗性」に着目し、言語モデルの持つテキスト圧縮の観点からその理由を分析
    具体的には、各学習段階における学習データ量をバネ定数として、圧縮率（予測性能）の変化がフックの法則に従うこと発見（つまり、大規模データで学習された分布の予測性能は損なわれにくいが、少量データで学習されたアラインメント情報は損なわれやすい）
    さらに、アラインメントに用いたデータ量が多いほど最初の応答は適切だが、逆アラインメントを行うと少量データでのアラインメント時以上に性能が低下する (rebound) ことを確認
    現状のOSS LLMは最小限の追加学習により容易にアラインメントを解除できることが明らかとなったため、より頑健なアラインメント手法の開発が求められる

- title: "Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing"
  category: "Speech"
  date: "2025-09-24"
  link: "https://arxiv.org/abs/2509.17052"
  note: >
    LoRAチューニングされたw2v-BERT 2.0によるノイズ除去+特徴量抽出と、HiFi-GANによる波形生成を組み合わせた音声復元手法Sidonを提案
    類似の構造（USM + WaveFit）を採用しているMiipher-2と比較してフルバンド復元・オープンソース・推論速度等の強みを持ち、品質でも同等以上を達成
    Miipher-2では残響・雑音・コーデック劣化の3種類の劣化を用いているのに対し、Sidonではさらに帯域不足・クリッピング・パケットロスを加えた6種類の劣化を用いており、より多様な劣化に対して頑健

- title: "Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens"
  category: "Multimodal (T/S)"
  date: "2025-09-23"
  link: "https://arxiv.org/abs/2509.14882"
  note: >
    MimiのRVQ音声トークンをflattenしてTransformer Decoderで直接モデル化した音声言語モデルLlama-Mimiを提案
    Mimiの1コードブック目はWavLMで蒸留されているためsemanticな情報も表現でき、系列長を抑えるため4コードブックを用いてモデル化（12.5Hz x 4 = 50 tokens/sec）
    Llama 3 1.3B/8Bを約24万時間の英語音声でfull FTし、音響的なタスクではGSLM, TWISTを上回る性能を達成
    一方で文脈的なタスクや生成され音声内容ではTWISTに及ばず、semantic/acousticのどちらを重視するかでトレードオフが存在

- title: "Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs"
  category: "LLM"
  date: "2025-09-23"
  link: "https://aclanthology.org/2025.acl-long.341.pdf"
  note: >
    近年のLLMの文脈では、人種や性別等を無視して公平な扱いすることが重視されていたが（difference unawareness）、法律や保護特性の扱いなど特定の文脈では必要な差別化することが望ましい
    本研究ではどのような文脈で差異を認めるべきか/無視するべきかを評価できるベンチマークと2つの評価指標DiffAware, CtxtAwareを提案
    DiffAwareは「差を認めるべき文脈で適切に差を認められた割合」、CtxtAwareは「差を認めたもののうち差を認めるべき文脈であった割合」を表し、それぞれrecall, precisionに相当
    9種類のモデルを用いた実験ではモデルの性能（MMLUスコアで代表）が上がるほどCtxtAwareは改善するが、DiffAwareは改善せず、文脈を考慮した適切な差別化の重要性を示唆している

- title: "A Theory of Response Sampling in LLMs: Part Descriptive and Part Prescriptive"
  category: "LLM"
  date: "2025-09-22"
  link: "https://aclanthology.org/2025.acl-long.1454.pdf"
  note: >
    LLMからサンプルされる値は、本来の統計的な値 (descriptive) と比較して、理想的とされる値 (prescriptive) の方に偏ることを示した論文
    15種類のLLMにさまざまなトピックに関する数量的な問いを投げかけ（例:1週間に飲まれる砂糖飲料の量は？）、平均値・理想値・純粋なサンプル値の3つを答えさせる実験により、上記の仮説の統計的有意性を確認

- title: "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"
  category: "LLM"
  date: "2025-09-21"
  link: "https://aclanthology.org/2025.acl-long.1126.pdf"
  note: >
    既存のsparse attention系手法は推論時にのみ適用されることが多く、学習を効率化できない、精度が低下する等の課題があった
    Native Sparse Attention (NSA)は学習時と推論時の両方で統一的なsparse化を実現し、Full Attentionと同等以上の精度を保ちながら学習・推論を効率化
    NSAのコアは (1)ブロックごとに圧縮するtoken compression (2)スコアの高いブロックを用いるtoken selection (3)直近のトークンを用いるsliding window を組み合わせた機構で、局所性と長距離依存の両方を捉える

- title: "MiMo-Audio: Audio Language Models are Few-Shot Learners"
  category: "Multimodal (T/S)"
  date: "2025-09-20"
  link: "https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf"
  note: >
    マルチモーダルな音声言語モデルを1億時間以上の音声データで学習することにより、タスク特化の微調整なしで音声QA、声質変換、音声翻訳等を実現できることを示した研究
    音声トークナイザーはTransformerを中心とした1.2Bパラメータのモデルで、25Hz x 8コードブックで音声を表現、(1)音声トークンの獲得 (2)再構成品質向上 を主目的として2段階学習
    言語モデル部分はMimo-7B-Baseを6層のPatch Encoder, 16層のPatch Decoderで拡張して音声トークンを6.25Hzで扱い、(1)音声理解 (2)音声理解・生成の2段階で事前学習
    さらにASR, TTS, Audio 音理解, 音声対話, 指示追従TTS, テキスト対話等のフォーマットで微調整されたMimo-Audio-7B-Instructは、MMAU, MMAU-Pro, MMAR, MMSU, Big Bench Audio, MultiChallenge Audio等のベンチマークでSoTAを達成

- title: "LLM-I: LLMs are Naturally Interleaved Multimodal Creators"
  category: "Multimodal (T/I)"
  date: "2025-09-19"
  link: "https://arxiv.org/abs/2509.13642"
  note: >
    LLMを用いて画像を含むコンテンツの生成を行う場合、従来のモデルは単一のツールしか利用できない課題があった
    本研究ではインターネット画像検索、Pythonによる図表生成、画像生成、画像編集の4つのツールをテキストと織り交ぜながら利用可能なLLM-Iを提案
    モデルは出力に含まれる画像数に関するルールベースの報酬と、LLMおよびMLLMによって計算される報酬を組み合わせて強化学習
    複数ツールを動的に使い分けることにより、ベンチマークでは従来のunified multimodal LLMを上回る性能を達成

- title: "WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching"
  category: "NV"
  date: "2025-09-18"
  link: "https://arxiv.org/abs/2503.16689"
  note: >
    フローマッチングを用いたニューラルボコーダWaveFMを提案
    拡散モデルベース（PriorGrad等）で用いられるメルスペクトログラムの振幅に応じた事前分布は安定化のためにクリッピングが必要であったが、WaveFMでは不要となりより適切な事前分布を設定可能に
    HiFi-GANのResBlockを取り入れたU-Netをフローマッチング損失+改良版MR-STFT損失+メルスペクトログラム損失で学習し、さらに1ステップで推論するためのConsistency Distillationも実施
    LibriTTSでの実験では1ステップの推論でもHiFi-GAN V1や拡散モデルベースNVを上回る品質とHiFi-GAN V1に近いRTFを実現

- title: "Single-stream Policy Optimization"
  category: "LLM"
  date: "2025-09-17"
  link: "https://arxiv.org/abs/2509.13232"
  note: >
    GRPOは複数サンプルの生成を要するため、全て正解/全て不正解の場合に学習信号が得られなかったり、一部サンプルで生成時間が長い（ツール呼び出し等）場合に全体のボトルネックになる課題があった
    Single-stream Policy Optimization (SPO) は1プロンプトにつき1サンプルのみ生成する代わりに、全プロンプトに対する過去の成功率を保持することで学習を安定化・効率化
    また過去の成功率が50%に近いプロンプトを優先的に用いるPrioritized Prompt Samplingによって学習信号の質を向上
    Qwen3-8Bを用いた実験では数学難問ベンチマーク (AIME 24/25, BRUMO 25 etc.) でGRPOよりも高い性能を達成
    対話・コード生成・一般QAなど他領域での優位性は未検証で、手法の設計的に大規模なRLHFには向かない可能性もあり、GRPOを完全に置き換えるものではなさそう

- title: "Improving diffusion inverse problem solving with decoupled noise annealing"
  category: "Image"
  date: "2025-09-16"
  link: "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing_CVPR_2025_paper.pdf"
  note: >
    劣化した観測からの復元を行うPosterior Samplingに拡散モデルを適用した従来法では、各ステップごとに徐々にサンプルを修正するため、初期のサンプリングで生じた誤差を修正できない課題があった
    本研究では毎回のステップで (1)x_0を逆拡散過程に従ってサンプルし (2)拡散過程に従ってx_tを生成し直すdecoupled noise annealingを提案し、誤差の修正を可能に
    線形な逆問題（deblurring, inpainting, etc.）だけでなく、非線形な逆問題（phase retrieval, nonlinear deblurring, etc.）でも高い性能を示し、Latent Diffusion Modelでも有効性を確認

- title: "Defeating Nondeterminism in LLM Inference"
  category: "LLM"
  date: "2025-09-15"
  link: "https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/"
  note: >
    LLMの推論がgreedy decodingにおいても非決定的であった問題が、バッチ非依存だと考えられていたRMSNorm, 行列積、Attentionなどの計算がバッチ依存であったことに起因していることを発見
    vLLM FlexAttention + torch.Libraryで決定論的な推論を実装し、実際に非決定性を排除できることを確認

- title: "Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models"
  category: "ML"
  date: "2025-09-14"
  link: "https://arxiv.org/abs/2410.11081"
  note: >
    時間ステップの小さい離散時間CMや連続時間CMの学習が不安定で大規模化も困難であったことから、以下を提案
    (1) Trigflowという三角関数に基づくスケジューリングによってモデルをシンプルかつ安定に
    (2) 学習を不安定にしているノイズスケジュール、時間埋め込み、正規化層を改良し、さらに正接正規化や適応的重みづけ等の工夫を導入
    (3) 1.5Bパラメータのモデルを学習し、ImageNet 512x512の2ステップ生成で教師モデルとのFID差を10%に抑えた

- title: "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs"
  category: "Multimodal (T/S)"
  date: "2025-09-13"
  link: "https://www.arxiv.org/abs/2509.09174"
  note: >
    従来のspeech-to-speech LLMにおける音声トークンをターゲットとした学習では、意味的には近いが音響的には遠い出力を適切にモデル化できず、テキスト能力の劣化が顕著であった
    echoXは、LLMの最終層の特徴量からgreedy decodingによって得られる擬似テキスト応答をもとにターゲットのunit language系列を作成し、これを予測するようにecho decoderを学習
    実験では3B, 8Bのモデルを6千時間程度でデータで学習し先行研究と同等以上の性能を達成し、さらに提案する学習方法がQA正答率の維持に有効であることを確認

- title: "Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation"
  category: "ST"
  date: "2025-09-13"
  link: "https://aclanthology.org/2025.findings-acl.75.pdf"
  note: >
    動的計画法に基づいて複数の音声ユニットをまとめたunit languageを利用してtextless STを行い、従来のASR+MT+TTSのパイプラインと並ぶ翻訳精度を達成
    話者や収録環境に依存しないnorm unitを音声ユニットとして用い、各音声ユニットの出現確率はコーパス内の出現頻度に基づいて計算
    学習時には中間表現から変換元言語・変換先言語のunit language系列を予測する補助損失を用いる

- title: "Soundwave: Less is More for Speech-Text Alignment in LLMs"
  category: "Multimodal (T/S)"
  date: "2025-09-13"
  link: "https://aclanthology.org/2025.acl-long.917.pdf"
  note: >
    音声言語モデルの音声エンコーダ、Adapterの後段に系列方向の情報集約を行うShrinking Adapterを導入することで、1万時間弱のデータでQwen2-Audioと並ぶ性能を達成
    学習は (1)CTCベースのASRでAlignment Adapter学習 (2)ASR, STでAlignment Adapter, Shrinking Adapter, LoRA学習 (3)Adapterを固定して多様なタスクでLoRA学習 の3段階
    Shrinking AdapterはCTCのピークのみを抽出し、その系列をクエリとしたcross-attentionにより抽出された系列との和を出力する構造

- title: "Continuous Audio Language Models"
  category: "Multimodal (T/S)"
  date: "2025-09-11"
  link: "https://www.arxiv.org/abs/2509.06926"
  note: >
    VAEの連続的な潜在表現をTransformerで予測する形のContinuous Audio Language Models (CALM)を提案し、音声生成・楽曲生成での有効性を示した研究
    Transformerの出力zからConsistency Headを用いて1ステップで連続潜在表現xを予測する構造で、MoshiのRQ Transformerと比較して高速に動作（サンプラーで12倍、全体で1.3倍の高速化）
    学習時はzの生成がボトルネックになることから、一つのzに対して複数のノイズレベルで損失を計算することで学習の効率化・安定化を実現
    楽曲生成ではさらに(1)Transformerの入力にノイズ付加 (2)過去数ステップのコンテキストを捉える小さなTransformerの併用が有効
    backbone Transformerで直接潜在表現を予測すると性能が下がるのかについては触れられていなかったが、MAR, CLEARなどの研究でも同様の構造が採用されているので連続表現の学習においては有効か

- title: "Step-Audio 2 Technical Report"
  category: "Multimodal (T/S)"
  date: "2025-09-10"
  link: "https://arxiv.org/abs/2507.16632"
  note: >
    音声・音理解に強く音声応答も可能な大規模音声言語モデルStep-Audio 2の技術報告
    本家のモデルサイズや構造の詳細は非公開だが、近い性能を達成した公開モデルStep-Audio 2 miniはQwen2-Audioの音声エンコーダ、Qwen2.5-7BのLLM、CosyVoice 2のS3 Tokenizer, CFM, HiFTNetを採用

- title: "AudioBench: A Universal Benchmark for Audio Large Language Models"
  category: "Multimodal (T/S)"
  date: "2025-09-09"
  link: "https://aclanthology.org/2025.naacl-long.218v2.pdf"
  note: >
    Audio LLMの評価のための8タスク26データセットからなる統一的なベンチマークを提案
    タスクは音声内容理解（ASR, SQA, Speech Instruction）、パラ言語理解（Accent/Gender/Emotion Recognition）、音環境理解（Audio Captioning, Audio-Scene QA）を含む
    SALMONN, Qwen-Audio, WavLLM, Qwen2-Audio (Audio LLMs) とWhisper+Llama-3-8B-Instruct (cascade) を比較した実験では、Audio LLMsが長い音声のASRに弱く、パラ言語・音環境理解に強い傾向

- title: "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance"
  category: "TTS"
  date: "2025-09-08"
  link: "https://arxiv.org/abs/2502.05236"
  note: >
    LFSCの音声トークン（21.5Hz, 8コードブック）に基づくEncoder-Decoder型のTTS, Koel-TTSを提案
    音声プロンプトを(1)デコーダにprefixする (2)話者特徴量をエンコーダ出力に加算する (3)音声エンコーダを別に用意してcross-attention の3通りで与えて実験し、(1)のzero-shot性能が最も高いことを確認
    さらに、難読文章を含む6万件弱のデータを用いたDPO/RPOと、推論時のclassifier-free guidanceにより、WER, SSIM, Squim-MOSを改善

- title: "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference"
  category: "NAC"
  date: "2025-09-08"
  link: "https://arxiv.org/abs/2508.05835"
  note: >
    LFSCを改良・軽量化したCNNベースのNAC, NanoCodecを提案し、Encoder/Decoderの因果性、フレームレート、ビットレートなどが品質に与える影響を分析
    (1)Encoderのcausal化は大きく劣化するが、Decoderはそうでもない (2)25Hzと12.5Hzでは近い品質だが、6.25Hzは大きく劣化 (3)ビットレートは品質（特にCER）に直結 などの知見
    Koel-TTSに適用した実験では、21.5Hzの条件で同ビットレートのLFSCと同等のMOS、より良いCER, TTFAを達成
    12.5Hzの条件ではCER. RTFがさらに改善したものの、未知話者に対する話者類似性が劣化

- title: "LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems"
  category: "Multimodal (T/S)"
  date: "2025-09-07"
  link: "https://arxiv.org/abs/2502.14145"
  note: >
    full-duplexな音声対話のため、0.5BのLMを対話管理特化に微調整したSemantic-VADを提案
    Semantic-VADは事前にAEC, VAD, ASRで処理された書き起こしを受け取り、システム側の4状態 (発話開始、発話継続、傾聴開始、傾聴継続) を予測
    発話開始/継続が予測された場合のみ応答生成用LLMを駆動することで全体の計算量を削減
    実験ではDuplexConv, RTTL-DGと比較して対話管理精度を大きく改善

- title: "Reinforcement Learning Enhanced Full-Duplex Spoken Dialogue Language Models for Conversational Interactions"
  category: "Multimodal (T/S)"
  date: "2025-09-07"
  link: "https://openreview.net/pdf?id=QbLbXz8Idp"
  note: >
    full-duplexな音声対話モデルを定量的に評価可能な報酬で強化学習するOnline Reinforcement with Interactive Speech Evaluation (ORISE)を提案
    ベースとなる音声対話モデルはTinyLlama-1.1BをPSLM的に拡張したもので、(1)ユーザー音声埋め込み (2)システムテキスト (3)システム音声トークンを受け取り、(2)(3)を出力する構造
    ORISEは(1)VADで検出されたユーザー・システムターン数の一致度 (2)割り込み/相槌の処理の適切さ (3)システムテキストと音声の一致度 の3つの報酬の和を最大化するように強化学習
    モデルは主にTTSベースの擬似音声対話データで学習され、ユーザー側の相槌を挿入することで割り込み/相槌の両方を適切に処理可能に
    Moshiと比較して応答品質を大きく改善したがGTとの品質差はまだ大きいため、LLMを音声対話に適用する過程での知識保持が課題

- title: "MixedG2P-T5: G2P-free Speech Synthesis for Mixed-script texts using Speech Self-Supervised Learning and Language Model"
  category: "TTS"
  date: "2025-09-07"
  link: "https://www.arxiv.org/abs/2509.01391"
  note: >
    (1)T5でraw textからspeech unit列を予測 (2)FastSpeech 2でメルスペクトログラムを予測 (3)ボコーダで波形生成 の3段階でG2PフリーなTTSを実現
    論文中では日本語のみでの実験だが多言語への拡張も展望として挙げられており、その場合はテキストトークナイザーをBPE等に変更する必要があるとのこと

- title: "Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation"
  category: "ASR"
  date: "2025-09-06"
  link: "https://aclanthology.org/2025.findings-acl.125.pdf"
  note: >
    固有名詞のリストから認識結果と関連度の高いエントリを検索し、ASRの生成的誤り訂正に適用した最初の研究
    さらに、OODデータを用いてASRの誤りパターンを用意して誤り訂正用LLMをLoRAチューニングすることで、ID/OOD両方で一貫して認識率を改善
    固有名詞検索は「音声認識結果(文)」と「各固有名詞(単語)」のSentenceBERT埋め込み間の類似度に基づいており、英語では動くようだが他言語では課題があるかも
    また誤り訂正LLMのベースモデルとしてLlama-2-Instruct (7B)を使っているのでオーバーヘッドは大きそう

- title: "Generative Annotation for ASR Named Entity Correction"
  category: "ASR"
  date: "2025-09-05"
  link: "https://www.arxiv.org/abs/2508.20700"
  note: >
    ASRの固有名詞誤認識を訂正する手法として、従来は (固有名詞, 音素列) のペアを用いた編集距離ベースの手法 (Phonetic-level Edit Distance: PED) が用いられてきた
    PEDベースの手法は認識結果が目標の固有名詞と大きく異なる場合、PEDも大きくなって編集が困難であった (例: midjourney→米德仲尼 のようなケース)
    本研究では音声特徴量ベースの固有名詞検索とAttention Encoder-Decoderによる生成的な誤り訂正を組み合わせることで、誤認識結果が表記上遠い場合でも頑健な修正を実現
    現状は検索にself-attentionを使っており固有名詞数が増えると遅延が問題になるため、将来的にvector search等に置換できるとよさそうとのこと

- title: "CLEAR: Continuous Latent Autoregressive Modeling for High-quality and Low-latency Speech Synthesis"
  category: "TTS"
  date: "2025-09-04"
  link: "https://arxiv.org/abs/2508.19098"
  note: >
    離散音声トークンを用いたAR型TTSは高品質だが長い系列を予測する必要があり、推論速度に課題がある
    本研究では、wav-VAEで抽出される連続潜在空間における自己回帰モデルCLEARを提案し、系列長を削減することでこの問題に対処
    連続潜在空間を得るためのwav-VAEは1秒間の16kHzの音声を7.8個x1024次元のベクトルに圧縮/復元するため、高い推論効率を実現
    ARモデルはTransformerの後段に各トークンに対して独立に動作するRectified Flow Headを結合することで連続潜在空間の予測精度を向上
    LibriHeavyを用いて学習し従来のAR型TTSと同等の品質を達成しつつRTFを改善、streamingでは遅延96msを達成

- title: "SpectroStream: A Versatile Neural Codec for General Audio"
  category: "NAC"
  date: "2025-09-03"
  link: "https://arxiv.org/abs/2508.05207"
  note: >
    汎用的な音の離散表現獲得のためのSoundStreamの後継NACとして、48kHzのステレオ波形を25Hz64コードブック（最大）で表現するSpectroStreamを提案
    Encoder, Decoderは100Hzのメルスペクトログラムを入出力とした2D Convベースのstreaming可能な構造で、QuantizerはRVQを使用
    同一のビットレートでDACよりも高いViSQOLおよび主観評価スコアを獲得しており、特に低ビットレート（~3kbps）での改善が顕著
    https://huggingface.co/google/magenta-realtime で学習済みモデルが公開されている

- title: "FastVLM: Efficient Vision Encoding for Vision Language Models"
  category: "Multimodal (T/I)"
  date: "2025-09-03"
  link: "https://arxiv.org/abs/2412.13303"
  note: >
    FastViTを改良したFastViTHDとQwen2-0.5B/1.5B/7Bと組み合わせたVLMを提案
    FastViTHDはダウンサンプリング層を追加しトークン数を1/4に削減しつつ、DWConvベースのプーリング層を追加することで高解像な特徴も保持
    画像トークン数の削減により画像のエンコード、LLMのprefillにかかる時間を削減し、TTFTを大幅に短縮
    デモではiPhone上でリアルタイムにVLMが動作する様子も見られる

- title: "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation"
  category: "Multimodal (T/S)"
  date: "2025-09-02"
  link: "https://arxiv.org/abs/2410.17799"
  note: >
    Qwen2.5-0.5Bをfull-duplexな音声対話モデルに拡張したOmniFlattenを提案
    学習は (1)約10万時間のASR/TTSで音声モダリティに適応 (2)SpeechGPTのようにhalf-duplex型学習 (3)ユーザー音声10トークンからシステムテキスト2トークンとシステム音声10トークンを生成するfull-duplex型学習 の3段階
    実験ではカリキュラム学習の有効性、Moshiよりも高い応答品質を確認したが、ベースモデルと比較した場合の品質低下、システムテキストを介さない場合にはさらなる品質低下も見られた
    平均応答時間はMoshiよりもさらに速い模様（システム側553ms→193ms、ユーザ側753ms→287ms）
    CosyVoiceの音声トークナイザーを用いているとあるがこれはnon-causalっぽいので、学習段階(1)(2)と(3)の不整合が問題にならないのか気になる

- title: "rStar2-Agent: Agentic Reasoning Technical Report"
  category: "LLM"
  date: "2025-09-02"
  link: "https://arxiv.org/abs/2508.20722"
  note: >
    外部ツール（Python）の呼び出しを含めたエージェント的な強化学習により、14Bパラメータのモデルで671BパラメータのDeepseek-R1を上回る数学能力を達成
    従来数学やコーディング分野で用いられていた出力の正誤で報酬を与える方法では、誤ったツール呼び出しを抑制できなかった
    これに対し、初めにバッチサイズ以上の応答を生成し、(1)正答できた応答は最善の1サンプルを選択 (2)誤った応答はランダムに残りのバッチを埋めるよう選択するResample on Correct (RoC)を提案
    強化学習は(1)8Kコンテキストでの簡潔な応答生成 (2)12Kコンテキストに拡張した応答生成 (3)難しい問題に絞っての学習 の3段階で実施し、計510ステップで完了

- title: "On the Theoretical Limitations of Embedding-Based Retrieval"
  category: "Text Embedding"
  date: "2025-09-01"
  link: "https://arxiv.org/abs/2508.21038"
  note: >
    テキスト埋め込みに基づく情報検索では、埋め込みの次元によって表現できる組み合わせの数が制限され、表現不可能な検索タスクが存在することを理論的に示した
    また上記の検証のために現実的で自由度が高い検索タスクからなるLIMITデータセットを提案し、最新の埋め込みを利用しても性能が上げられないことを確認した

- title: "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling"
  category: "NAC"
  date: "2025-08-31"
  link: "https://arxiv.org/abs/2508.16790"
  note: >
    デコーダにテキスト情報を与えることで、6.25Hzと超低ビットレートながら高い音声再構成品質を実現
    エンコーダ・デコーダともdiffusion transformerベースで、後段でメルスペクトログラム→波形への変換も必要なため、リアルタイム応用に課題があるかも

- title: "SimPO: Simple Preference Optimization with a Reference-Free Reward"
  category: "LLM"
  date: "2025-08-30"
  link: "https://arxiv.org/abs/2405.14734"
  note: >
    DPOは人間の嗜好を反映する強化学習手法として広く用いられているが、参照モデルと現在のモデルの両方で対数尤度を計算する必要があった
    SimPOは参照モデルを用いず、系列長による正規化と良い回答・悪い回答間のマージンを導入することでより効率的な学習を実現
    Mistral, Llama 3, Gemma2を用いた実験ではDPO以上の性能を達成

- title: "Prompt-Guided Turn-Taking Prediction"
  category: "Speech"
  date: "2025-08-30"
  link: "https://arxiv.org/abs/2506.21191"
  note: >
    VAPモデルのSAの前後にテキストプロンプトの埋め込みを結合することで、「リズムよく返答する」「話し出す前にポーズを入れる」といったターンテイキングのテキスト制御を実現

- title: "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM"
  category: "Multimodal (T/S)"
  date: "2025-08-30"
  link: "https://arxiv.org/abs/2411.00774"
  note: >
    LLMのパラメータを固定して音声エンコーダ・音声デコーダ・ターンテイキング推定モジュールを学習した音声対話モデルを提案
    音声入力は (1)音声認識でエンコーダ学習 (2)Adapter/LLMを結合して音声認識で学習 (3)マルチターンQAで学習 の3段階
    音声出力は (1)TiCodec (40Hz, 1024コード) 学習 (2)LLMのembeddingから音声トークンを予測するよう学習 (3)LLMの中間特徴量を考慮して音声トークンを予測するよう学習 の3段階
    SpeechGPT, Spectron, Moshi, GLM-4-Voiceよりも高品質な応答を~1.2秒で生成可能

- title: "Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities"
  category: "Multimodal (T/S)"
  date: "2025-08-29"
  link: "https://arxiv.org/abs/2503.04721"
  note: >
    ポーズ、相槌、ターンテイキング、遮りといった双方向音声対話の特徴を捉えるためのデータセットおよび評価手法を提案
    データセットはCandor (850時間, ポーズ・ターンテイキング評価用), ICC (28分, 相槌評価用), 合成データ (337サンプル, ポーズ・遮り評価用) から成る
    cascade型のFreeze-Omniはs2s型のdGSLM, Moshiと比較してユーザー発話の遮りが少なく応答も適切だが、ターンの切り替わりが遅く相槌も少なめと評価された

- title: "VoxDialogue: Can Spoken Dialogue Systems Understand Information Beyond Words?"
  category: "Multimodal (T/S)"
  date: "2025-08-29"
  link: "https://openreview.net/forum?id=vbmSSIhKAM"
  note: >
    音声対話モデルが文脈だけでなく、話速・音量・強調・背景音といった音声特有の側面を考慮できているかを評価するための、4,500件のマルチターン対話データを提案
    ASRベースのシステム (FunAudioLLM) は文脈に沿った応答ができていた一方で、音声の特徴の把握が必要な対話ではASRフリーなシステム (Qwen2-Audio等) に軍配
    WavRewardの前身となるような研究のよう（著者の所属も浙江大学で共通）

- title: "Predicting the Order of Upcoming Tokens Improves Language Modeling"
  category: "LLM"
  date: "2025-08-29"
  link: "https://arxiv.org/abs/2508.19228"
  note: >
    未来のトークンを複数予測するMulti-Token Prediction (MTP)は難しすぎるという仮説から、各トークンが次に出現するまでの距離に基づくランクを予測するToken Order Prediction (TOP)を提案
    340M~7Bのモデルを用いた実験では、MTPは評価指標によっては性能劣化につながることがあったが、TOPは一貫して改善が見られた

- title: "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators"
  category: "Multimodal (T/S)"
  date: "2025-08-28"
  link: "https://arxiv.org/abs/2505.09558"
  note: >
    音声対話モデルを強化学習するためのデータセットChatReward-30Kと、それを用いてQwen2.5-Omni-7B-Thinkを強化学習した報酬モデルWavRewardを提案
    データセットは応答の適切さだけでなく、話し方、話者性、感情等の音声特有の側面も考慮して1, 3, 5の3段階でスコアリング
    音声対話のスコアリングにおいてGPT-4o-audioを超える性能を達成

- title: "LIST: Language-Independent Speech Token for Multilingual Speech Synthesis with Language Models"
  category: "TTS"
  date: "2025-08-28"
  link: "https://www.isca-archive.org/interspeech_2025/liu25o_interspeech.pdf"
  note: >
    ASRで学習される音声トークナイザーに対し、ASRのターゲットをIPAにすることで言語非依存な音声トークンを獲得する手法を提案
    音声トークンからメルスペクトログラムを生成するflow matching部分は声質変換された音声から抽出された音声トークンを入力として学習することで話者非依存な音声トークンを獲得
    TTSによる評価では、ASRのターゲットがテキスト（BPE）の場合と比較して、英語以外の全言語のCERを改善

- title: "OpusLM: A Family of Open Unified Speech Language Models"
  category: "Multimodal (T/S)"
  date: "2025-08-28"
  link: "https://arxiv.org/abs/2506.17611"
  note: >
    モデル、学習・推論コード、データを全て公開した1.3B/7Bの音声言語モデルを公開
    モデルはsemantic token x 1, acoustic token x 8を一度に予測するマルチストリームな構造
    ASR, TTS, NLPで高い性能を示すが、135Mや360Mでは大幅な性能劣化が見られ、モデルサイズの重要性を示唆

- title: "Scalable Spontaneous Speech Dataset (SSSD): Crowdsourcing Data Collection to Promote Dialogue Research"
  category: "Dataset (Speech)"
  date: "2025-08-27"
  link: "https://www.isca-archive.org/interspeech_2025/sheikh25_interspeech.pdf"
  note: "クラウドソーシングで2話者の自発的対話を収集した700時間超のデータセットを公開"

- title: "Is Synthetic Data Truly Effective for Training Speech Language Models?"
  category: "Multimodal (T/S)"
  date: "2025-08-20"
  link: "https://www.isca-archive.org/interspeech_2025/mizumoto25_interspeech.pdf"

- title: "AC/DC: LLM-based Audio Comprehension via Dialogue Continuation"
  category: "Multimodal (T/S)"
  date: "2025-08-19"
  link: "https://www.isca-archive.org/interspeech_2025/fujita25b_interspeech.pdf"

- title: "Group Sequence Policy Optimization"
  category: "LLM"
  date: "2025-07-31"
  link: "https://arxiv.org/abs/2507.18071"

- title: "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities"
  category: "Multimodal (T/S/I/V)"
  date: "2025-07-30"
  link: "https://arxiv.org/abs/2507.06261"

- title: "Towards a Japanese Full-duplex Spoken Dialogue System"
  category: "Multimodal (T/S)"
  date: "2025-06-30"
  link: "https://arxiv.org/abs/2506.02979"

- title: "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary"
  category: "ASR"
  date: "2025-06-15"
  link: "https://arxiv.org/abs/2506.09448"

- title: "HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset"
  category: "Dataset (Speech)"
  date: "2025-06-05"
  link: "https://arxiv.org/abs/2506.04152"

- title: "CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training"
  category: "TTS"
  date: "2025-05-28"
  link: "https://arxiv.org/abs/2505.17589"

- title: "Understanding R1-Zero-Like Training: A Critical Perspective"
  category: "LLM"
  date: "2025-05-27"
  link: "https://arxiv.org/abs/2503.20783"

- title: "How Do Large Language Models Acquire Factual Knowledge During Pretraining?"
  category: "LLM"
  date: "2025-05-16"
  link: "https://arxiv.org/abs/2406.11813"

- title: "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot"
  category: "Multimodal (T/S)"
  date: "2025-05-15"
  link: "https://arxiv.org/abs/2412.02612"

- title: "Qwen3: Think Deeper, Act Faster"
  category: "LLM"
  date: "2025-04-30"
  link: "https://qwenlm.github.io/blog/qwen3/"

- title: "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning"
  category: "LLM"
  date: "2025-04-28"
  link: "https://arxiv.org/abs/2412.03104"

- title: "Context-aware Dynamic Pruning for Speech Foundation Models"
  category: "Speech"
  tags: ["ASR", "TTS"]
  date: "2025-04-23"
  link: "https://openreview.net/forum?id=u2QdCiOgwA"

- title: "An Evolved Universal Transformer Memory"
  category: "LLM"
  date: "2025-04-22"
  link: "https://arxiv.org/abs/2410.13166"

- title: "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
  category: "LLM"
  date: "2025-04-18"
  link: "https://arxiv.org/abs/2306.00978"

- title: "ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling"
  category: "NAC"
  date: "2025-04-16"
  link: "https://arxiv.org/abs/2504.10344"

- title: "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space"
  category: "LLM"
  date: "2025-04-15"
  link: "https://arxiv.org/abs/2402.12865"

- title: "A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication"
  category: "NAC"
  date: "2025-04-14"
  link: "https://arxiv.org/abs/2504.06561"

- title: "Understanding R1-Zero-Like Training: A Critical Perspective"
  category: "LLM"
  date: "2025-04-11"
  link: "https://arxiv.org/abs/2503.20783"

- title: "VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation"
  category: "Multimodal (T/S)"
  tags: ["LLM", "TTS"]
  date: "2025-04-10"
  link: "https://arxiv.org/abs/2504.04060"

- title: "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation"
  category: "LLM"
  date: "2025-04-07"
  link: "https://ai.meta.com/blog/llama-4-multimodal-intelligence/"

- title: "MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis"
  category: "TTS"
  date: "2025-04-03"
  link: "https://arxiv.org/abs/2502.18924"

- title: "MoCha: Towards Movie-Grade Talking Character Synthesis"
  category: "THG"
  date: "2025-04-01"
  link: "https://arxiv.org/abs/2503.23307"

- title: "Scaling Transformers for Low-Bitrate High-Quality Speech Coding"
  category: "TTS"
  date: "2025-03-31"
  link: "https://arxiv.org/abs/2411.19842"

- title: "Qwen2.5-Omni Technical Report"
  category: "Multimodal (T/S/I/V)"
  date: "2025-03-27"
  link: "https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf"

- title: "STFTCodec: High-Fidelity Audio Compression through Time-Frequency Domain Representation"
  category: "NAC"
  date: "2025-03-26"
  link: "https://arxiv.org/abs/2503.16989"

- title: "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis"
  category: "TTS"
  date: "2025-03-25"
  link: "https://arxiv.org/abs/2502.04128"

- title: "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound"
  category: "NAC"
  date: "2025-03-24"
  link: "https://arxiv.org/abs/2502.04128"

- title: "Gemma 3 Technical Report"
  category: "LLM"
  date: "2025-03-14"
  link: "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf"

- title: "UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining"
  category: "LLM"
  date: "2025-03-13"
  link: "https://arxiv.org/abs/2304.09151"

- title: "ESPnet-SDS: Unified Toolkit and Demo for Spoken Dialogue Systems"
  category: "Multimodal (T/S)"
  date: "2025-03-12"
  link: "https://arxiv.org/abs/2503.08533"

- title: "FlowDec: A flow-based full-band general audio codec with high perceptual quality"
  category: "NAC"
  date: "2025-03-11"
  link: "https://arxiv.org/abs/2503.01485"

- title: "Good practices for evaluation of synthesized speech"
  category: "TTS"
  date: "2025-03-10"
  link: "https://arxiv.org/abs/2503.03250"

- title: "Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics"
  category: "Multimodal (T/S)"
  date: "2025-03-06"
  link: "https://arxiv.org/abs/2503.01174"

- title: "Chain of Draft: Thinking Faster by Writing Less"
  category: "LLM"
  date: "2025-03-04"
  link: "https://arxiv.org/abs/2501.15907"

- title: "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation"
  category: "TTI"
  date: "2025-02-27"
  link: "https://arxiv.org/abs/2502.18364"

- title: "Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision"
  category: "ML"
  date: "2025-02-26"
  link: "https://arxiv.org/abs/2109.08203"

- title: "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation"
  category: "ML"
  date: "2025-02-25"
  link: "https://arxiv.org/abs/2406.05298"

- title: "Spectral Codecs: Spectrogram-Based Audio Codecs for High Quality Speech Synthesis"
  category: "NAC"
  date: "2025-02-20"
  link: "https://arxiv.org/abs/2406.05298"

- title: "Emo-DPO: Controllable Emotional Speech Synthesis through Direct Preference Optimization"
  category: "TTS"
  date: "2025-02-19"
  link: "https://arxiv.org/abs/2409.10157"

- title: "SpeechAlign: Aligning Speech Generation to Human Preferences"
  category: "TTS"
  date: "2025-02-18"
  link: "https://arxiv.org/abs/2404.05600"

- title: "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference"
  category: "NAC"
  date: "2025-02-17"
  link: "https://arxiv.org/abs/2410.21951"

- title: "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding"
  category: "TTS"
  date: "2025-02-13"
  link: "https://arxiv.org/abs/2410.21951"

- title: "Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for Speech, Music, and Sound"
  category: "Audio"
  date: "2025-02-12"
  link: "https://arxiv.org/abs/2502.05139"

- title: "s1: Simple test-time scaling"
  category: "LLM"
  date: "2025-02-10"
  link: "https://arxiv.org/abs/2501.19393"

- title: "High-Fidelity Simultaneous Speech-To-Speech Translation"
  category: "ST"
  date: "2025-02-07"
  link: "https://arxiv.org/abs/2502.03382"

- title: "Wavelet-based Positional Representation for Long Context"
  category: "LLM"
  date: "2025-02-06"
  link: "https://arxiv.org/abs/2502.02004"

- title: "Investigating the Impact of Incremental Processing and Voice Activity Projection on Spoken Dialogue Systems"
  category: "Multimodal (T/S)"
  date: "2025-02-05"
  link: "https://aclanthology.org/2025.coling-main.249/"

- title: "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data"
  category: "TTS"
  date: "2025-02-04"
  link: "https://arxiv.org/abs/2402.08093"

- title: "Finite Scalar Quantization: VQ-VAE Made Simple"
  category: "ML"
  date: "2025-02-03"
  link: "https://arxiv.org/abs/2309.15505"

- title: "Sigmoid Loss for Language Image Pre-Training"
  category: "Multimodal (T/I)"
  date: "2025-01-31"
  link: "https://arxiv.org/abs/2303.15343"

- title: "TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models"
  category: "LLM"
  date: "2025-01-30"
  link: "https://arxiv.org/abs/2501.16937"

- title: "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
  category: "LLM"
  date: "2025-01-29"
  link: "https://arxiv.org/abs/2501.12948"

- title: "Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling"
  category: "Multimodal (T/I)"
  date: "2025-01-28"
  link: "https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf"

- title: "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models"
  category: "TTS"
  date: "2025-01-27"
  link: "https://arxiv.org/abs/2412.10117"

- title: "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction"
  category: "LLM"
  date: "2025-01-23"
  link: "https://arxiv.org/abs/2501.06282"

- title: "YaRN: Efficient Context Window Extension of Large Language Models"
  category: "LLM"
  date: "2025-01-22"
  link: "https://arxiv.org/abs/2309.00071"

- title: "DeepSeek-V3 Technical Report"
  category: "LLM"
  date: "2025-01-10"
  link: "https://arxiv.org/abs/2412.19437"

- title: "Phi-4 Technical Report"
  category: "LLM"
  date: "2025-01-09"
  link: "https://arxiv.org/abs/2412.08905"

- title: "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching"
  category: "TTS"
  date: "2025-01-08"
  link: "https://arxiv.org/abs/2410.06885"

- title: "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models"
  category: "TTS"
  date: "2025-01-07"
  link: "https://arxiv.org/abs/2306.07691"

- title: "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
  category: "LLM"
  date: "2025-01-06"
  link: "https://arxiv.org/abs/2402.03300"

- title: "Flow Matching for Generative Modeling"
  category: "ML"
  date: "2024-12-28"
  link: "https://arxiv.org/abs/2210.02747"

- title: "Matcha-TTS: A fast TTS architecture with conditional flow matching"
  category: "TTS"
  date: "2024-12-27"
  link: "https://arxiv.org/abs/2309.03199"

- title: "GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech"
  category: "Dataset (Speech)"
  date: "2024-12-26"
  link: "https://arxiv.org/abs/2406.14875"

- title: "SONAR: Sentence-Level Multimodal and Language-Agnostic Representations"
  category: "Multimodal (T/S)"
  date: "2024-12-25"
  link: "https://arxiv.org/abs/2308.11466"

- title: "Large Concept Models: Language Modeling in a Sentence Representation Space"
  category: "LLM"
  date: "2024-12-24"
  link: "https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/?utm_source=twitter&utm_medium=organic_social&utm_content=thread&utm_campaign=fair"

- title: "SQ-Whisper: Speaker-Querying based Whisper Model for Target-Speaker ASR"
  category: "ASR"
  date: "2024-12-10"
  link: "https://arxiv.org/abs/2412.05589"

- title: "Towards Robust Speech Representation Learning for Thousands of Languages"
  category: "Speech"
  date: "2024-11-14"
  link: "https://aclanthology.org/2024.emnlp-main.570/"

- title: "BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec"
  category: "NAC"
  date: "2024-11-13"
  link: "https://arxiv.org/abs/2409.05377"

- title: "Language Modeling Is Compression"
  category: "LLM"
  date: "2024-11-08"
  link: "https://arxiv.org/abs/2309.10668"

- title: "A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion"
  category: "VC"
  date: "2024-11-07"
  link: "https://arxiv.org/abs/2111.02392"

- title: "Knowledge Distillation from Self-Supervised Representation Learning Model with Discrete Speech Units for Any-to-Any Streaming Voice Conversion"
  category: "VC"
  date: "2024-11-06"
  link: "https://www.isca-archive.org/interspeech_2024/kanagawa24b_interspeech.pdf"

- title: "The Impact of Positional Encoding on Length Generalization in Transformers"
  category: "LLM"
  date: "2024-11-05"
  link: "https://arxiv.org/abs/2305.19466"

- title: "Very Attentive Tacotron: Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech"
  category: "TTS"
  date: "2024-11-01"
  link: "https://arxiv.org/abs/2410.22179"

- title: "Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities"
  category: "Multimodal (T/S/I)"
  date: "2024-10-18"
  link: "https://arxiv.org/abs/2410.11190"

- title: "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis"
  category: "TTS"
  date: "2024-10-12"
  link: "https://arxiv.org/abs/2410.04380"

- title: "J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling"
  category: "Dataset (Speech)"
  date: "2024-10-11"
  link: "https://arxiv.org/abs/2407.15828"

- title: "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"
  category: "LLM"
  date: "2024-10-10"
  link: "https://arxiv.org/abs/2409.15594"

- title: "Moshi: a speech-text foundation model for real-time dialogue"
  category: "Multimodal (T/S)"
  date: "2024-09-20"
  link: "https://arxiv.org/abs/2410.00037"

- title: "Super Monotonic Alignment Search"
  category: "TTS"
  date: "2024-09-17"
  link: "https://arxiv.org/abs/2409.07704"

- title: "LLaMA-Omni: Seamless Speech Interaction with Large Language Models"
  category: "Multimodal (T/S)"
  date: "2024-09-16"
  link: "https://arxiv.org/abs/2409.06666"

- title: "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications"
  category: "TTS"
  date: "2024-09-11"
  link: "https://arxiv.org/abs/2409.03283"

- title: "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming"
  category: "Multimodal (T/S)"
  date: "2024-09-10"
  link: "https://arxiv.org/abs/2408.16725"

- title: "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling"
  category: "NAC"
  date: "2024-09-09"
  link: "https://arxiv.org/abs/2408.16532"
